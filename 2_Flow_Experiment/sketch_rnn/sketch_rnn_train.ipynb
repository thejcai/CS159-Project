{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Collecting numpy==1.16.2\n",
      "  Using cached https://files.pythonhosted.org/packages/c4/33/8ec8dcdb4ede5d453047bbdbd01916dbaccdb63e98bba60989718f5f0876/numpy-1.16.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.16.2\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2019 The Magenta Authors.\n",
    "#\n",
    "\n",
    "\"\"\"SketchRNN training.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "# from magenta.models.sketch_rnn import model as sketch_rnn_model\n",
    "# from magenta.models.sketch_rnn import utils\n",
    "import model as sketch_rnn_model\n",
    "import utils\n",
    "\n",
    "!pip install numpy==1.16.2\n",
    "import numpy as np\n",
    "import requests\n",
    "import six\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'data_dir',\n",
    "    'https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep',\n",
    "    'The directory in which to find the dataset specified in model hparams. '\n",
    "    'If data_dir starts with \"http://\" or \"https://\", the file will be fetched '\n",
    "    'remotely.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'log_root', 'log/initial/', # '/tmp/sketch_rnn/models/default',\n",
    "    'Directory to store model checkpoints, tensorboard.')\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'resume_training', True,\n",
    "    'Set to true to load previous checkpoint')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'hparams', '',\n",
    "    'Pass in comma-separated key=value pairs such as '\n",
    "    '\\'save_every=40,decay_rate=0.99\\' '\n",
    "    '(no whitespace) to be read into the HParams object defined in model.py')\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "\n",
    "PRETRAINED_MODELS_URL = ('http://download.magenta.tensorflow.org/models/'\n",
    "                         'sketch_rnn.zip')\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    \"\"\"Closes the current default session and resets the graph.\"\"\"\n",
    "    sess = tf.get_default_session()\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def load_env(data_dir, model_dir):\n",
    "    \"\"\"Loads environment for inference mode, used in jupyter notebook.\"\"\"\n",
    "    model_params = sketch_rnn_model.get_default_hparams()\n",
    "    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
    "        model_params.parse_json(f.read())\n",
    "    return load_dataset(data_dir, model_params, inference_mode=True)\n",
    "\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"Loads model for inference mode, used in jupyter notebook.\"\"\"\n",
    "    model_params = sketch_rnn_model.get_default_hparams()\n",
    "    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
    "        model_params.parse_json(f.read())\n",
    "\n",
    "    model_params.batch_size = 1  # only sample one at a time\n",
    "    eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n",
    "    eval_model_params.use_input_dropout = 0\n",
    "    eval_model_params.use_recurrent_dropout = 0\n",
    "    eval_model_params.use_output_dropout = 0\n",
    "    eval_model_params.is_training = 0\n",
    "    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n",
    "    sample_model_params.max_seq_len = 1  # sample one point at a time\n",
    "    return [model_params, eval_model_params, sample_model_params]\n",
    "\n",
    "\n",
    "def download_pretrained_models(\n",
    "    models_root_dir='/tmp/sketch_rnn/models',\n",
    "    pretrained_models_url=PRETRAINED_MODELS_URL):\n",
    "    \"\"\"Download pretrained models to a temporary directory.\"\"\"\n",
    "    tf.gfile.MakeDirs(models_root_dir)\n",
    "    zip_path = os.path.join(\n",
    "        models_root_dir, os.path.basename(pretrained_models_url))\n",
    "    if os.path.isfile(zip_path):\n",
    "        tf.logging.info('%s already exists, using cached copy', zip_path)\n",
    "    else:\n",
    "        tf.logging.info('Downloading pretrained models from %s...',\n",
    "                        pretrained_models_url)\n",
    "        urlretrieve(pretrained_models_url, zip_path)\n",
    "        tf.logging.info('Download complete.')\n",
    "    tf.logging.info('Unzipping %s...', zip_path)\n",
    "    with zipfile.ZipFile(zip_path) as models_zip:\n",
    "        models_zip.extractall(models_root_dir)\n",
    "    tf.logging.info('Unzipping complete.')\n",
    "\n",
    "\n",
    "def load_dataset(data_dir, model_params, inference_mode=False):\n",
    "    \"\"\"Loads the .npz file, and splits the set into train/valid/test.\"\"\"\n",
    "\n",
    "    # normalizes the x and y columns using the training set.\n",
    "    # applies same scaling factor to valid and test set.\n",
    "\n",
    "    if isinstance(model_params.data_set, list):\n",
    "        datasets = model_params.data_set\n",
    "    else:\n",
    "        datasets = [model_params.data_set]\n",
    "\n",
    "    train_strokes = None\n",
    "    valid_strokes = None\n",
    "    test_strokes = None\n",
    "\n",
    "    for dataset in datasets:\n",
    "        if data_dir.startswith('http://') or data_dir.startswith('https://'):\n",
    "            data_filepath = '/'.join([data_dir, dataset])\n",
    "            tf.logging.info('Downloading %s', data_filepath)\n",
    "            response = requests.get(data_filepath)\n",
    "            data = np.load(six.BytesIO(response.content), allow_pickle=True, encoding='latin1')\n",
    "        else:\n",
    "            data_filepath = os.path.join(data_dir, dataset)\n",
    "            if six.PY3:\n",
    "                data = np.load(data_filepath, allow_pickle=True, encoding='latin1')\n",
    "            else:\n",
    "                data = np.load(data_filepath, allow_pickle=True)\n",
    "        tf.logging.info('Loaded {}/{}/{} from {}'.format(\n",
    "            len(data['train']), len(data['valid']), len(data['test']),\n",
    "            dataset))\n",
    "        if train_strokes is None:\n",
    "            train_strokes = data['train']\n",
    "            valid_strokes = data['valid']\n",
    "            test_strokes = data['test']\n",
    "        else:\n",
    "            train_strokes = np.concatenate((train_strokes, data['train']))\n",
    "            valid_strokes = np.concatenate((valid_strokes, data['valid']))\n",
    "            test_strokes = np.concatenate((test_strokes, data['test']))\n",
    "\n",
    "    all_strokes = np.concatenate((train_strokes, valid_strokes, test_strokes))\n",
    "    num_points = 0\n",
    "    for stroke in all_strokes:\n",
    "        num_points += len(stroke)\n",
    "    avg_len = num_points / len(all_strokes)\n",
    "    tf.logging.info('Dataset combined: {} ({}/{}/{}), avg len {}'.format(\n",
    "        len(all_strokes), len(train_strokes), len(valid_strokes),\n",
    "        len(test_strokes), int(avg_len)))\n",
    "\n",
    "    # calculate the max strokes we need.\n",
    "    max_seq_len = utils.get_max_len(all_strokes)\n",
    "    # overwrite the hps with this calculation.\n",
    "    model_params.max_seq_len = max_seq_len\n",
    "\n",
    "    tf.logging.info('model_params.max_seq_len %i.', model_params.max_seq_len)\n",
    "\n",
    "    eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n",
    "\n",
    "    eval_model_params.use_input_dropout = 0\n",
    "    eval_model_params.use_recurrent_dropout = 0\n",
    "    eval_model_params.use_output_dropout = 0\n",
    "    eval_model_params.is_training = 1\n",
    "\n",
    "    if inference_mode:\n",
    "        eval_model_params.batch_size = 1\n",
    "        eval_model_params.is_training = 0\n",
    "\n",
    "    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n",
    "    sample_model_params.batch_size = 1  # only sample one at a time\n",
    "    sample_model_params.max_seq_len = 1  # sample one point at a time\n",
    "\n",
    "    train_set = utils.DataLoader(\n",
    "        train_strokes,\n",
    "        model_params.batch_size,\n",
    "        max_seq_length=model_params.max_seq_len,\n",
    "        random_scale_factor=model_params.random_scale_factor,\n",
    "        augment_stroke_prob=model_params.augment_stroke_prob)\n",
    "\n",
    "    normalizing_scale_factor = train_set.calculate_normalizing_scale_factor()\n",
    "    train_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "    valid_set = utils.DataLoader(\n",
    "        valid_strokes,\n",
    "        eval_model_params.batch_size,\n",
    "        max_seq_length=eval_model_params.max_seq_len,\n",
    "        random_scale_factor=0.0,\n",
    "        augment_stroke_prob=0.0)\n",
    "    valid_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "    test_set = utils.DataLoader(\n",
    "        test_strokes,\n",
    "        eval_model_params.batch_size,\n",
    "        max_seq_length=eval_model_params.max_seq_len,\n",
    "        random_scale_factor=0.0,\n",
    "        augment_stroke_prob=0.0)\n",
    "    test_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "    tf.logging.info('normalizing_scale_factor %4.4f.', normalizing_scale_factor)\n",
    "\n",
    "    result = [\n",
    "        train_set, valid_set, test_set, model_params, eval_model_params,\n",
    "        sample_model_params\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_model(sess, model, data_set):\n",
    "    \"\"\"Returns the average weighted cost, reconstruction cost and KL cost.\"\"\"\n",
    "    total_cost = 0.0\n",
    "    total_r_cost = 0.0\n",
    "    total_kl_cost = 0.0\n",
    "    for batch in range(data_set.num_batches):\n",
    "        unused_orig_x, x, s = data_set.get_batch(batch)\n",
    "        feed = {model.input_data: x, model.sequence_lengths: s}\n",
    "        (cost, r_cost,\n",
    "         kl_cost) = sess.run([model.cost, model.r_cost, model.kl_cost], feed)\n",
    "        total_cost += cost\n",
    "        total_r_cost += r_cost\n",
    "        total_kl_cost += kl_cost\n",
    "\n",
    "    total_cost /= (data_set.num_batches)\n",
    "    total_r_cost /= (data_set.num_batches)\n",
    "    total_kl_cost /= (data_set.num_batches)\n",
    "    return (total_cost, total_r_cost, total_kl_cost)\n",
    "\n",
    "\n",
    "def load_checkpoint(sess, checkpoint_path):\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "    tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "\n",
    "def save_model(sess, model_save_path, global_step):\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint_path = os.path.join(model_save_path, 'vector')\n",
    "    tf.logging.info('saving model %s.', checkpoint_path)\n",
    "    tf.logging.info('global_step %i.', global_step)\n",
    "    saver.save(sess, checkpoint_path, global_step=global_step)\n",
    "\n",
    "\n",
    "def train(sess, model, eval_model, train_set, valid_set, test_set):\n",
    "    \"\"\"Train a sketch-rnn model.\"\"\"\n",
    "    # Setup summary writer.\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_root)\n",
    "\n",
    "    # Calculate trainable params.\n",
    "    t_vars = tf.trainable_variables()\n",
    "    count_t_vars = 0\n",
    "    for var in t_vars:\n",
    "        num_param = np.prod(var.get_shape().as_list())\n",
    "        count_t_vars += num_param\n",
    "        tf.logging.info('%s %s %i', var.name, str(var.get_shape()), num_param)\n",
    "    tf.logging.info('Total trainable variables %i.', count_t_vars)\n",
    "    model_summ = tf.summary.Summary()\n",
    "    model_summ.value.add(\n",
    "          tag='Num_Trainable_Params', simple_value=float(count_t_vars))\n",
    "    summary_writer.add_summary(model_summ, 0)\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # setup eval stats\n",
    "    best_valid_cost = 100000000.0  # set a large init value\n",
    "    valid_cost = 0.0\n",
    "\n",
    "    # main train loop\n",
    "\n",
    "    hps = model.hps\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(hps.num_steps):\n",
    "\n",
    "        step = sess.run(model.global_step)\n",
    "\n",
    "        curr_learning_rate = ((hps.learning_rate - hps.min_learning_rate) *\n",
    "                              (hps.decay_rate)**step + hps.min_learning_rate)\n",
    "        curr_kl_weight = (hps.kl_weight - (hps.kl_weight - hps.kl_weight_start) *\n",
    "                          (hps.kl_decay_rate)**step)\n",
    "\n",
    "        _, x, s = train_set.random_batch()\n",
    "        feed = {\n",
    "            model.input_data: x,\n",
    "            model.sequence_lengths: s,\n",
    "            model.lr: curr_learning_rate,\n",
    "            model.kl_weight: curr_kl_weight\n",
    "        }\n",
    "\n",
    "        (train_cost, r_cost, kl_cost, _, train_step, _) = sess.run([\n",
    "            model.cost, model.r_cost, model.kl_cost, model.final_state,\n",
    "            model.global_step, model.train_op\n",
    "        ], feed)\n",
    "\n",
    "        if step % 20 == 0 and step > 0:\n",
    "\n",
    "            end = time.time()\n",
    "            time_taken = end - start\n",
    "\n",
    "            cost_summ = tf.summary.Summary()\n",
    "            cost_summ.value.add(tag='Train_Cost', simple_value=float(train_cost))\n",
    "            reconstr_summ = tf.summary.Summary()\n",
    "            reconstr_summ.value.add(\n",
    "                tag='Train_Reconstr_Cost', simple_value=float(r_cost))\n",
    "            kl_summ = tf.summary.Summary()\n",
    "            kl_summ.value.add(tag='Train_KL_Cost', simple_value=float(kl_cost))\n",
    "            lr_summ = tf.summary.Summary()\n",
    "            lr_summ.value.add(\n",
    "                tag='Learning_Rate', simple_value=float(curr_learning_rate))\n",
    "            kl_weight_summ = tf.summary.Summary()\n",
    "            kl_weight_summ.value.add(\n",
    "                tag='KL_Weight', simple_value=float(curr_kl_weight))\n",
    "            time_summ = tf.summary.Summary()\n",
    "            time_summ.value.add(\n",
    "                tag='Time_Taken_Train', simple_value=float(time_taken))\n",
    "\n",
    "            output_format = ('step: %d, lr: %.6f, klw: %0.4f, cost: %.4f, '\n",
    "                             'recon: %.4f, kl: %.4f, train_time_taken: %.4f')\n",
    "            output_values = (step, curr_learning_rate, curr_kl_weight, train_cost,\n",
    "                             r_cost, kl_cost, time_taken)\n",
    "            output_log = output_format % output_values\n",
    "\n",
    "            tf.logging.info(output_log)\n",
    "\n",
    "            summary_writer.add_summary(cost_summ, train_step)\n",
    "            summary_writer.add_summary(reconstr_summ, train_step)\n",
    "            summary_writer.add_summary(kl_summ, train_step)\n",
    "            summary_writer.add_summary(lr_summ, train_step)\n",
    "            summary_writer.add_summary(kl_weight_summ, train_step)\n",
    "            summary_writer.add_summary(time_summ, train_step)\n",
    "            summary_writer.flush()\n",
    "            start = time.time()\n",
    "\n",
    "        if step % hps.save_every == 0 and step > 0:\n",
    "\n",
    "            (valid_cost, valid_r_cost, valid_kl_cost) = evaluate_model(\n",
    "                sess, eval_model, valid_set)\n",
    "\n",
    "            end = time.time()\n",
    "            time_taken_valid = end - start\n",
    "            start = time.time()\n",
    "\n",
    "            valid_cost_summ = tf.summary.Summary()\n",
    "            valid_cost_summ.value.add(\n",
    "                tag='Valid_Cost', simple_value=float(valid_cost))\n",
    "            valid_reconstr_summ = tf.summary.Summary()\n",
    "            valid_reconstr_summ.value.add(\n",
    "                tag='Valid_Reconstr_Cost', simple_value=float(valid_r_cost))\n",
    "            valid_kl_summ = tf.summary.Summary()\n",
    "            valid_kl_summ.value.add(\n",
    "                tag='Valid_KL_Cost', simple_value=float(valid_kl_cost))\n",
    "            valid_time_summ = tf.summary.Summary()\n",
    "            valid_time_summ.value.add(\n",
    "                tag='Time_Taken_Valid', simple_value=float(time_taken_valid))\n",
    "\n",
    "            output_format = ('best_valid_cost: %0.4f, valid_cost: %.4f, valid_recon: '\n",
    "                             '%.4f, valid_kl: %.4f, valid_time_taken: %.4f')\n",
    "            output_values = (min(best_valid_cost, valid_cost), valid_cost,\n",
    "                             valid_r_cost, valid_kl_cost, time_taken_valid)\n",
    "            output_log = output_format % output_values\n",
    "\n",
    "            tf.logging.info(output_log)\n",
    "\n",
    "            summary_writer.add_summary(valid_cost_summ, train_step)\n",
    "            summary_writer.add_summary(valid_reconstr_summ, train_step)\n",
    "            summary_writer.add_summary(valid_kl_summ, train_step)\n",
    "            summary_writer.add_summary(valid_time_summ, train_step)\n",
    "            summary_writer.flush()\n",
    "\n",
    "            if valid_cost < best_valid_cost:\n",
    "                best_valid_cost = valid_cost\n",
    "\n",
    "                save_model(sess, FLAGS.log_root, step)\n",
    "\n",
    "                end = time.time()\n",
    "                time_taken_save = end - start\n",
    "                start = time.time()\n",
    "\n",
    "                tf.logging.info('time_taken_save %4.4f.', time_taken_save)\n",
    "\n",
    "                best_valid_cost_summ = tf.summary.Summary()\n",
    "                best_valid_cost_summ.value.add(\n",
    "                    tag='Best_Valid_Cost', simple_value=float(best_valid_cost))\n",
    "\n",
    "                summary_writer.add_summary(best_valid_cost_summ, train_step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "                (eval_cost, eval_r_cost, eval_kl_cost) = evaluate_model(\n",
    "                    sess, eval_model, test_set)\n",
    "\n",
    "                end = time.time()\n",
    "                time_taken_eval = end - start\n",
    "                start = time.time()\n",
    "\n",
    "                eval_cost_summ = tf.summary.Summary()\n",
    "                eval_cost_summ.value.add(tag='Eval_Cost', simple_value=float(eval_cost))\n",
    "                eval_reconstr_summ = tf.summary.Summary()\n",
    "                eval_reconstr_summ.value.add(\n",
    "                    tag='Eval_Reconstr_Cost', simple_value=float(eval_r_cost))\n",
    "                eval_kl_summ = tf.summary.Summary()\n",
    "                eval_kl_summ.value.add(\n",
    "                    tag='Eval_KL_Cost', simple_value=float(eval_kl_cost))\n",
    "                eval_time_summ = tf.summary.Summary()\n",
    "                eval_time_summ.value.add(\n",
    "                    tag='Time_Taken_Eval', simple_value=float(time_taken_eval))\n",
    "\n",
    "                output_format = ('eval_cost: %.4f, eval_recon: %.4f, '\n",
    "                                 'eval_kl: %.4f, eval_time_taken: %.4f')\n",
    "                output_values = (eval_cost, eval_r_cost, eval_kl_cost, time_taken_eval)\n",
    "                output_log = output_format % output_values\n",
    "\n",
    "                tf.logging.info(output_log)\n",
    "\n",
    "                summary_writer.add_summary(eval_cost_summ, train_step)\n",
    "                summary_writer.add_summary(eval_reconstr_summ, train_step)\n",
    "                summary_writer.add_summary(eval_kl_summ, train_step)\n",
    "                summary_writer.add_summary(eval_time_summ, train_step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "\n",
    "def trainer(model_params):\n",
    "    \"\"\"Train a sketch-rnn model.\"\"\"\n",
    "    np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)\n",
    "\n",
    "    tf.logging.info('sketch-rnn')\n",
    "    tf.logging.info('Hyperparams:')\n",
    "    for key, val in six.iteritems(model_params.values()):\n",
    "        tf.logging.info('%s = %s', key, str(val))\n",
    "    tf.logging.info('Loading data files.')\n",
    "    datasets = load_dataset(FLAGS.data_dir, model_params)\n",
    "\n",
    "    train_set = datasets[0]\n",
    "    valid_set = datasets[1]\n",
    "    test_set = datasets[2]\n",
    "    model_params = datasets[3]\n",
    "    eval_model_params = datasets[4]\n",
    "\n",
    "    reset_graph()\n",
    "    model = sketch_rnn_model.Model(model_params)\n",
    "    eval_model = sketch_rnn_model.Model(eval_model_params, reuse=True)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if FLAGS.resume_training:\n",
    "        load_checkpoint(sess, FLAGS.log_root)\n",
    "\n",
    "    # Write config file to json file.\n",
    "    tf.gfile.MakeDirs(FLAGS.log_root)\n",
    "    with tf.gfile.Open(\n",
    "          os.path.join(FLAGS.log_root, 'model_config.json'), 'w') as f:\n",
    "        json.dump(model_params.values(), f, indent=True)\n",
    "\n",
    "    train(sess, model, eval_model, train_set, valid_set, test_set)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "    \"\"\"Load model params, save config file and start trainer.\"\"\"\n",
    "    model_params = sketch_rnn_model.get_default_hparams()\n",
    "    if FLAGS.hparams:\n",
    "        model_params.parse(FLAGS.hparams)\n",
    "    trainer(model_params)\n",
    "\n",
    "\n",
    "def console_entry_point():\n",
    "    tf.app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:sketch-rnn\n",
      "INFO:tensorflow:Hyperparams:\n",
      "INFO:tensorflow:dec_model = layer_norm\n",
      "INFO:tensorflow:num_mixture = 20\n",
      "INFO:tensorflow:conditional = True\n",
      "INFO:tensorflow:min_learning_rate = 1e-05\n",
      "INFO:tensorflow:kl_weight_start = 0.01\n",
      "INFO:tensorflow:recurrent_dropout_prob = 0.9\n",
      "INFO:tensorflow:input_dropout_prob = 0.9\n",
      "INFO:tensorflow:data_set = ['aaron_sheep.npz']\n",
      "INFO:tensorflow:max_seq_len = 250\n",
      "INFO:tensorflow:random_scale_factor = 0.15\n",
      "INFO:tensorflow:num_flows = 2\n",
      "INFO:tensorflow:augment_stroke_prob = 0.1\n",
      "INFO:tensorflow:grad_clip = 1.0\n",
      "INFO:tensorflow:z_size = 128\n",
      "INFO:tensorflow:decay_rate = 0.9999\n",
      "INFO:tensorflow:use_recurrent_dropout = True\n",
      "INFO:tensorflow:use_output_dropout = False\n",
      "INFO:tensorflow:use_input_dropout = False\n",
      "INFO:tensorflow:is_training = True\n",
      "INFO:tensorflow:kl_tolerance = 0.2\n",
      "INFO:tensorflow:learning_rate = 0.001\n",
      "INFO:tensorflow:dec_rnn_size = 512\n",
      "INFO:tensorflow:kl_decay_rate = 0.99995\n",
      "INFO:tensorflow:kl_weight = 0.25\n",
      "INFO:tensorflow:enc_model = layer_norm\n",
      "INFO:tensorflow:batch_size = 100\n",
      "INFO:tensorflow:save_every = 500\n",
      "INFO:tensorflow:num_steps = 100000\n",
      "INFO:tensorflow:enc_rnn_size = 256\n",
      "INFO:tensorflow:output_dropout_prob = 0.9\n",
      "INFO:tensorflow:Loading data files.\n",
      "INFO:tensorflow:Downloading https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\n",
      "INFO:tensorflow:Loaded 7400/300/300 from aaron_sheep.npz\n",
      "INFO:tensorflow:Dataset combined: 8000 (7400/300/300), avg len 125\n",
      "INFO:tensorflow:model_params.max_seq_len 250.\n",
      "total images <= max_seq_len is 7400\n",
      "total images <= max_seq_len is 300\n",
      "total images <= max_seq_len is 300\n",
      "INFO:tensorflow:normalizing_scale_factor 18.5198.\n",
      "INFO:tensorflow:Model using gpu.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Input dropout mode = False.\n",
      "INFO:tensorflow:Output dropout mode = False.\n",
      "INFO:tensorflow:Recurrent dropout mode = True.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:90: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/rnn.py:288: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:320: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:339: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:349: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Model using gpu.\n",
      "INFO:tensorflow:Input dropout mode = 0.\n",
      "INFO:tensorflow:Output dropout mode = 0.\n",
      "INFO:tensorflow:Recurrent dropout mode = 0.\n",
      "INFO:tensorflow:Loading model log/initial/vector-127500.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from log/initial/vector-127500\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/fw/LayerNormLSTMCell/W_xh:0 (5, 1024) 5120\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/fw/LayerNormLSTMCell/W_hh:0 (256, 1024) 262144\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/fw/LayerNormLSTMCell/ln_all/ln_gamma:0 (1024,) 1024\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/fw/LayerNormLSTMCell/ln_all/ln_beta:0 (1024,) 1024\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/fw/LayerNormLSTMCell/ln_c/ln_gamma:0 (256,) 256\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/fw/LayerNormLSTMCell/ln_c/ln_beta:0 (256,) 256\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/bw/LayerNormLSTMCell/W_xh:0 (5, 1024) 5120\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/bw/LayerNormLSTMCell/W_hh:0 (256, 1024) 262144\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/bw/LayerNormLSTMCell/ln_all/ln_gamma:0 (1024,) 1024\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/bw/LayerNormLSTMCell/ln_all/ln_beta:0 (1024,) 1024\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/bw/LayerNormLSTMCell/ln_c/ln_gamma:0 (256,) 256\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN/bw/LayerNormLSTMCell/ln_c/ln_beta:0 (256,) 256\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_mu/super_linear_w:0 (512, 128) 65536\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_mu/super_linear_b:0 (128,) 128\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_sigma/super_linear_w:0 (512, 128) 65536\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_sigma/super_linear_b:0 (128,) 128\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_u_0/super_linear_w:0 (512, 128) 65536\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_u_0/super_linear_b:0 (128,) 128\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_w_0/super_linear_w:0 (512, 128) 65536\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_w_0/super_linear_b:0 (128,) 128\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_b_0/super_linear_w:0 (512, 1) 512\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_b_0/super_linear_b:0 (1,) 1\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_u_1/super_linear_w:0 (512, 128) 65536\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_u_1/super_linear_b:0 (128,) 128\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_w_1/super_linear_w:0 (512, 128) 65536\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_w_1/super_linear_b:0 (128,) 128\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_b_1/super_linear_w:0 (512, 1) 512\n",
      "INFO:tensorflow:vector_rnn/ENC_RNN_b_1/super_linear_b:0 (1,) 1\n",
      "INFO:tensorflow:vector_rnn/linear/super_linear_w:0 (128, 1024) 131072\n",
      "INFO:tensorflow:vector_rnn/linear/super_linear_b:0 (1024,) 1024\n",
      "INFO:tensorflow:vector_rnn/RNN/output_w:0 (512, 123) 62976\n",
      "INFO:tensorflow:vector_rnn/RNN/output_b:0 (123,) 123\n",
      "INFO:tensorflow:vector_rnn/RNN/LayerNormLSTMCell/W_xh:0 (133, 2048) 272384\n",
      "INFO:tensorflow:vector_rnn/RNN/LayerNormLSTMCell/W_hh:0 (512, 2048) 1048576\n",
      "INFO:tensorflow:vector_rnn/RNN/LayerNormLSTMCell/ln_all/ln_gamma:0 (2048,) 2048\n",
      "INFO:tensorflow:vector_rnn/RNN/LayerNormLSTMCell/ln_all/ln_beta:0 (2048,) 2048\n",
      "INFO:tensorflow:vector_rnn/RNN/LayerNormLSTMCell/ln_c/ln_gamma:0 (512,) 512\n",
      "INFO:tensorflow:vector_rnn/RNN/LayerNormLSTMCell/ln_c/ln_beta:0 (512,) 512\n",
      "INFO:tensorflow:Total trainable variables 2455933.\n",
      "INFO:tensorflow:step: 127520, lr: 0.000010, klw: 0.2496, cost: -0.5476, recon: -0.5975, kl: 0.2000, train_time_taken: 153.0629\n",
      "INFO:tensorflow:step: 127540, lr: 0.000010, klw: 0.2496, cost: -0.6567, recon: -0.7066, kl: 0.2000, train_time_taken: 143.8026\n",
      "INFO:tensorflow:step: 127560, lr: 0.000010, klw: 0.2496, cost: -0.6950, recon: -0.7450, kl: 0.2000, train_time_taken: 146.9684\n"
     ]
    }
   ],
   "source": [
    "console_entry_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation():\n",
    "    # Load model parameters\n",
    "    model_params = sketch_rnn_model.get_default_hparams()\n",
    "    if FLAGS.hparams:\n",
    "        model_params = sketch_rnn_model.get_default_hparams()\n",
    "    \n",
    "    # Load dataset\n",
    "    datasets = load_dataset(FLAGS.data_dir, model_params)\n",
    "    train_set = datasets[0]\n",
    "    valid_set = datasets[1]\n",
    "    test_set = datasets[2]\n",
    "    model_params = datasets[3]\n",
    "    eval_model_params = datasets[4]\n",
    "    \n",
    "    # Construct the model\n",
    "    reset_graph()\n",
    "    model = sketch_rnn_model.Model(model_params)\n",
    "    eval_model = sketch_rnn_model.Model(eval_model_params, reuse=True)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if FLAGS.resume_training:\n",
    "        load_checkpoint(sess, FLAGS.log_root)\n",
    "\n",
    "    # Evaluate against train, val, test\n",
    "    (train_cost, train_r_cost, train_kl_cost) = evaluate_model(\n",
    "                sess, eval_model, train_set)\n",
    "    (valid_cost, valid_r_cost, valid_kl_cost) = evaluate_model(\n",
    "                sess, eval_model, valid_set)\n",
    "    (test_cost, test_r_cost, test_kl_cost) = evaluate_model(\n",
    "                sess, eval_model, test_set)\n",
    "    output_format = 'eval_cost: %.4f, eval_recon: %.4f, eval_kl: %.4f'\n",
    "    print(output_format % (train_cost, train_r_cost, train_kl_cost))\n",
    "    print(output_format % (valid_cost, valid_r_cost, valid_kl_cost))\n",
    "    print(output_format % (test_cost, test_r_cost, test_kl_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Downloading https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\n",
      "INFO:tensorflow:Loaded 7400/300/300 from aaron_sheep.npz\n",
      "INFO:tensorflow:Dataset combined: 8000 (7400/300/300), avg len 125\n",
      "INFO:tensorflow:model_params.max_seq_len 250.\n",
      "total images <= max_seq_len is 7400\n",
      "total images <= max_seq_len is 300\n",
      "total images <= max_seq_len is 300\n",
      "INFO:tensorflow:normalizing_scale_factor 18.5198.\n",
      "INFO:tensorflow:Model using gpu.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Input dropout mode = False.\n",
      "INFO:tensorflow:Output dropout mode = False.\n",
      "INFO:tensorflow:Recurrent dropout mode = True.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:90: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/rnn.py:288: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:320: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:339: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jupyter/sketch_rnn/model.py:349: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Model using gpu.\n",
      "INFO:tensorflow:Input dropout mode = 0.\n",
      "INFO:tensorflow:Output dropout mode = 0.\n",
      "INFO:tensorflow:Recurrent dropout mode = 0.\n",
      "INFO:tensorflow:Loading model log/initial/vector-139000.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from log/initial/vector-139000\n",
      "eval_cost: -0.7242, eval_recon: -0.7264, eval_kl: 0.2142\n",
      "eval_cost: -0.5237, eval_recon: -0.5258, eval_kl: 0.2143\n",
      "eval_cost: -0.5211, eval_recon: -0.5233, eval_kl: 0.2171\n"
     ]
    }
   ],
   "source": [
    "get_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
